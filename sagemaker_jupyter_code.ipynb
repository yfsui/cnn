{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Convolution1D, GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"embeddings_dictionary_size\": 500000,\n",
    "    \"embeddings_vector_size\": 25,\n",
    "    \"padding_size\": 20,\n",
    "    \"batch_size\": 1000,\n",
    "    \"embeddings_path\": \"glove.txt\",\n",
    "    \"input_tensor_name\": \"embedding_input\",\n",
    "    \"num_epoch\":10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to read data from S3\n",
    "def read_data(path,mode):\n",
    "           \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    path_split = path.replace(\"s3://\", \"\").split(\"/\")\n",
    "\n",
    "    bucket = path_split.pop(0)\n",
    "    key = \"/\".join(path_split)\n",
    "\n",
    "    data = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "    decoded_file = data[\"Body\"].read().decode('utf-8').split('\\n')\n",
    "    \n",
    "    features=[]; sentiment=[]\n",
    "    for line in decoded_file:\n",
    "        content = json.loads(line)\n",
    "        features.append(content['features'])\n",
    "        sentiment.append(content[\"sentiment\"]/4)  \n",
    "    \n",
    "    num_data_points = len(features)\n",
    "    num_batches = math.ceil(len(features)/config['batch_size'])\n",
    "    \n",
    "    Dataset = tf.data.Dataset\n",
    "    \n",
    "    dataset = Dataset.from_tensor_slices((features, sentiment))\n",
    "\n",
    "    if mode == \"train\":\n",
    "\n",
    "        dataset = Dataset.from_tensor_slices((features, sentiment))\n",
    "        dataset = dataset.batch(config[\"batch_size\"]).shuffle(10000, seed=12345).repeat(\n",
    "            config[\"num_epoch\"])\n",
    "        num_batches = math.ceil(len(features) / config[\"batch_size\"])\n",
    "\n",
    "    if mode in (\"validation\", \"eval\"):\n",
    "\n",
    "        dataset = dataset.batch(config[\"batch_size\"]).repeat(config[\"num_epoch\"])\n",
    "        num_batches = int(math.ceil(len(features) / config[\"batch_size\"]))\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    dataset_features, dataset_sentiments = iterator.get_next()\n",
    "\n",
    "\n",
    "    return [{config[\"input_tensor_name\"]: dataset_features}, dataset_sentiments,\n",
    "            {\"num_data_point\": num_data_points, \"num_batches\": num_batches}]\n",
    "\n",
    "#read data from S3\n",
    "train_dataset=read_data('s3://ai-assignment/assignment4/train_data/train.json','train')\n",
    "eval_dataset=read_data('s3://ai-assignment/assignment4/eval_data/eval.json','eval')\n",
    "dev_dataset=read_data('s3://ai-assignment/assignment4/dev_data/dev.json','validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to read embedding dictionary with embeddings_path\n",
    "def read_dictionary(path,embeddings_dictionary_size,embeddings_vector_size):\n",
    "    \n",
    "    embedding_matrix = np.zeros((embeddings_dictionary_size, embeddings_vector_size))\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    path_split = path.replace(\"s3://\", \"\").split(\"/\")\n",
    "\n",
    "    bucket = path_split.pop(0)\n",
    "    key = \"/\".join(path_split)\n",
    "\n",
    "    data = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "    decoded_file = data[\"Body\"].read().decode('utf-8').split('\\n')\n",
    "    \n",
    "    for i in range(embeddings_dictionary_size):\n",
    "        if len(decoded_file[i].split()[1:]) != embeddings_vector_size:\n",
    "            continue\n",
    "        embedding_matrix[i] = np.asarray(decoded_file[i].split()[1:], dtype='float32')\n",
    "           \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN model\n",
    "def keras_model_fn(_, config):\n",
    "    \"\"\"\n",
    "    Creating a CNN model for sentiment modeling\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_matrix = read_dictionary('s3://ai-assignment/assignment4/glove.txt',config[\"embeddings_dictionary_size\"],config[\"embeddings_vector_size\"])\n",
    "\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(weights=[embedding_matrix], input_length = config[\"padding_size\"],input_dim = config[\"embeddings_dictionary_size\"],output_dim = config[\"embeddings_vector_size\"], trainable = True))\n",
    "    cnn_model.add(Convolution1D(filters=100,kernel_size=2,strides = 1, padding='valid',activation = 'relu'))\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    cnn_model.add(Dense(units=100, activation = 'relu'))\n",
    "    cnn_model.add(Dense(units=1, activation = 'sigmoid'))\n",
    "    Adam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    cnn_model.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics =['accuracy'])\n",
    "\n",
    "    return cnn_model\n",
    "\n",
    "#save CNN model into S3\n",
    "def save_model(model):\n",
    "    \n",
    "    model.save('s3://ai-assignment/assignment4/output/sentiment_model.h5')\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.meta.client.upload_file('assignment4/output/sentiment_model.h5','ai-assignment','sentiment_model.h5')\n",
    "    \n",
    "    print(\"Model successfully saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main function to fit model\n",
    "def main(train_dataset,validation_dataset,eval_dataset):\n",
    "    \"\"\"\n",
    "    Main training method\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preparing for training...\")\n",
    "\n",
    "    training_config = config    \n",
    "\n",
    "    model = keras_model_fn(None, training_config)\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    model.fit(\n",
    "        x=train_dataset[0]['embedding_input'], y=train_dataset[1], steps_per_epoch=train_dataset[2][\"num_batches\"],\n",
    "        epochs=training_config[\"num_epoch\"],\n",
    "        validation_data=(validation_dataset[0]['embedding_input'], validation_dataset[1]),\n",
    "        validation_steps=validation_dataset[2][\"num_batches\"])\n",
    "\n",
    "    score = model.evaluate(\n",
    "        eval_dataset[0]['embedding_input'], eval_dataset[1], steps=eval_dataset[2][\"num_batches\"], verbose=0)\n",
    "\n",
    "    print(\"Test loss:{}\".format(score[0]))\n",
    "    print(\"Test accuracy:{}\".format(score[1]))\n",
    "    \n",
    "#     save_model(model)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for training...\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Starting training...\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 3s 340ms/step - loss: 0.8139 - acc: 0.5024 - val_loss: 0.7697 - val_acc: 0.4930\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.6974 - acc: 0.5452 - val_loss: 0.6895 - val_acc: 0.5280\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 3s 289ms/step - loss: 0.6722 - acc: 0.5749 - val_loss: 0.6661 - val_acc: 0.6130\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 3s 288ms/step - loss: 0.6495 - acc: 0.6359 - val_loss: 0.6524 - val_acc: 0.6300\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.6284 - acc: 0.6708 - val_loss: 0.6353 - val_acc: 0.6460\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 0.6051 - acc: 0.6851 - val_loss: 0.6182 - val_acc: 0.6530\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 3s 285ms/step - loss: 0.5821 - acc: 0.6988 - val_loss: 0.6063 - val_acc: 0.6570\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 3s 289ms/step - loss: 0.5591 - acc: 0.7127 - val_loss: 0.5980 - val_acc: 0.6650\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.5394 - acc: 0.7225 - val_loss: 0.5922 - val_acc: 0.6730\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 3s 286ms/step - loss: 0.5207 - acc: 0.7368 - val_loss: 0.5885 - val_acc: 0.6960\n",
      "Test loss:0.5747805237770081\n",
      "Test accuracy:0.7039999961853027\n"
     ]
    }
   ],
   "source": [
    "model = main(train_dataset,dev_dataset,eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
